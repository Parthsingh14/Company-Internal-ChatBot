---

# ğŸ§  Company Internal Chatbot (RAG-Based Knowledge Assistant)

A full-stack **Retrieval-Augmented Generation (RAG)** powered internal company assistant built using **Next.js, TypeScript, Pinecone, Gemini (Embeddings), and Groq (LLM)**.

This project simulates how enterprise internal AI assistants are built to provide accurate, grounded responses based strictly on company documentation.

---

## ğŸš€ Project Overview

This system enables employees to query internal company documents using natural language while ensuring:

* âœ… Grounded responses (no hallucinated answers)
* âœ… Secure, admin-controlled document management
* âœ… Vector-based semantic retrieval
* âœ… Production-style AI architecture

Unlike traditional chatbots, this system does **not rely solely on LLM memory**. Instead, it retrieves relevant document chunks from a vector database and uses them as context for answer generation.

---

# ğŸ—ï¸ Architecture

## ğŸ”¹ Stage 1 â€” Document Indexing (Admin-Only Process)

```text
PDF Document
   â†“
LangChain PDFLoader
   â†“
Text Chunking (500 tokens, 100 overlap)
   â†“
Gemini Embedding (3072-dimension vectors)
   â†“
Pinecone (Dense Vector Storage)
```

* Indexing is a **one-time process per document**
* Re-run only if:

  * A new document is added
  * Document content changes
  * Embedding model changes
  * Index is reset

---

## ğŸ”¹ Stage 2 â€” Query Flow (RAG Pipeline)

```text
User Question
   â†“
Generate Embedding (Gemini)
   â†“
Pinecone Similarity Search (Top K = 3)
   â†“
Retrieve Relevant Chunks
   â†“
Groq LLM (Llama 3.1 8B)
   â†“
Grounded Answer
```

The LLM is strictly instructed to:

> Answer only using retrieved context. If not found, refuse.

This reduces hallucinations and ensures enterprise reliability.

---

# ğŸ› ï¸ Tech Stack

## Frontend

* Next.js (App Router)
* React
* TypeScript
* Tailwind CSS

## Backend

* Next.js API Routes
* TypeScript

## AI & Vector Stack

* **Gemini API** â†’ Embeddings (`gemini-embedding-001`)
* **Pinecone** â†’ Vector Database (Dense, 3072 dimension)
* **Groq API** â†’ LLM (Llama 3.1 8B Instant)
* **LangChain** â†’ Text Chunking (RecursiveCharacterTextSplitter)

---

# ğŸ“‚ Project Structure

```
app/
 â”œâ”€â”€ page.tsx              â†’ Chat UI
 â””â”€â”€ api/
      â””â”€â”€ llmchat/route.ts â†’ RAG API

scripts/
 â”œâ”€â”€ prepare.ts            â†’ Document indexing script
 â”œâ”€â”€ embedding.ts          â†’ Gemini embedding logic
 â””â”€â”€ vectorStore.ts        â†’ Pinecone operations

data/documents/
 â””â”€â”€ nexus.pdf             â†’ Company knowledge base
```

---

# ğŸ”’ Document Management & Security

This system is designed for **internal company usage**.

### ğŸš« End users cannot upload documents.

* Knowledge base updates are admin-controlled
* Indexing is a manual process
* Prevents unauthorized document injection
* Ensures data integrity

---

## ğŸ“Œ Adding a New Document

1ï¸âƒ£ Place the new file inside:

```
data/documents/
```

2ï¸âƒ£ Run the indexing script manually:

```bash
npx tsx scripts/prepare.ts
```

This script will:

* Load the document
* Chunk the content
* Generate embeddings (Gemini)
* Store vectors in Pinecone

âš ï¸ The chatbot will only retrieve data from documents that have already been indexed.

---

# ğŸ”Œ API Design

## POST `/api/llmchat`

### Request

```json
{
  "question": "What is the leave policy?"
}
```

### Internal Flow

1. Create embedding for question
2. Query Pinecone for top 3 similar vectors
3. Build contextual prompt
4. Send to Groq LLM
5. Return response

### Response

```json
{
  "reply": "The company provides 18 paid leaves per year..."
}
```

---

# ğŸ” Environment Variables

Create `.env.local`:

```
GEMINI_API_KEY=your_gemini_key
PINECONE_API_KEY=your_pinecone_key
GROQ_API_KEY=your_groq_key
```

Next.js automatically loads environment variables (no dotenv required).

---

# â–¶ï¸ Running the Project

### Install dependencies

```bash
npm install
```

### Run development server

```bash
npm run dev
```

Visit:

```
http://localhost:3000
```

---

# ğŸ§ª Example Queries

* What is the leave policy?
* Who is the backend team lead?
* What are working hours?
* How to bake a cake? â†’ (Correctly refused)

---

# âš™ï¸ Key Challenges Faced

### 1ï¸âƒ£ Gemini SDK Version Conflicts

* v1 vs v1beta API confusion
* Model naming inconsistencies
* Embedding vs generation endpoint mismatch

**Resolution:**
Used Gemini strictly for embeddings and switched to Groq for generation.

---

### 2ï¸âƒ£ Pinecone Vector Shape Errors

* Incorrect embedding return format
* Upsert schema mismatch
* Namespace query structure errors

**Resolution:**
Ensured embedding returns `number[]` (3072 dimensions) and validated before query.

---

### 3ï¸âƒ£ Dependency Conflicts

* dotenv peer dependency conflicts
* LangChain + Stagehand dependency mismatch

**Resolution:**
Removed unnecessary dotenv usage (Next.js handles env internally).

---

### 4ï¸âƒ£ RAG Debugging Complexity

* Incorrect JSON destructuring in API route
* Passing object instead of string to embedding function
* SDK response shape misunderstandings

**Resolution:**
Strict TypeScript typing + structured debugging.

---

# ğŸ§  What This Project Demonstrates

* Real-world RAG system architecture
* Vector database integration
* Embedding lifecycle management
* Controlled document ingestion
* Prompt engineering for hallucination reduction
* Production-style debugging
* Dependency conflict resolution

This mirrors how enterprise internal AI assistants are built.

---

# ğŸš€ Future Improvements

* Streaming LLM responses
* Conversational memory
* Admin-only document upload panel
* Role-based access control (RBAC)
* Citation references in answers
* Multi-document indexing
* Hybrid search (keyword + vector)

---

# ğŸ† Why This Matters

This is not just a chatbot.

It demonstrates:

* Understanding of AI system architecture
* End-to-end RAG pipeline implementation
* Secure internal knowledge management
* Production-aware design decisions

---

## ğŸ‘¨â€ğŸ’» Author

Built as a practical implementation of enterprise-grade Retrieval-Augmented Generation architecture.

---
