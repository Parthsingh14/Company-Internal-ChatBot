Perfect ğŸ‘
This README needs to look **professional, technical, and recruiter-friendly** â€” not tutorial-level.

Iâ€™ll give you a polished version you can directly replace your current README with.

---

# ğŸ“„ README.md (Replace Your Entire File With This)

---

# ğŸ§  Company Internal Chatbot (RAG-Based)

A full-stack **Retrieval-Augmented Generation (RAG)** based internal company chatbot built using **Next.js, TypeScript, Pinecone, Gemini (Embeddings), and Groq (LLM)**.

This project simulates a real-world enterprise internal assistant that answers questions strictly based on company documents.

---

## ğŸš€ Overview

This chatbot allows users to query internal company documents (PDF) using natural language.

Instead of relying purely on LLM knowledge, it:

1. Converts documents into vector embeddings
2. Stores them in a vector database (Pinecone)
3. Retrieves relevant chunks based on user queries
4. Sends retrieved context to an LLM (Groq)
5. Returns grounded, context-aware responses

This ensures:

* Factual responses
* No hallucinations (controlled via prompt)
* Enterprise-ready architecture

---

## ğŸ—ï¸ Tech Stack

### Frontend

* Next.js (App Router)
* React
* TypeScript
* Tailwind CSS

### Backend

* Next.js API Routes
* TypeScript

### AI & Vector Stack

* **Gemini API** â†’ Embeddings (`gemini-embedding-001`)
* **Pinecone** â†’ Vector Database
* **Groq API** â†’ LLM (Llama 3.1 8B Instant)
* LangChain â†’ Text splitting (RecursiveCharacterTextSplitter)

---

## ğŸ§  Architecture Flow

### ğŸ”¹ Stage 1: Indexing (One-Time Process)

```
PDF Document
   â†“
LangChain PDFLoader
   â†“
Text Chunking (500 tokens, 100 overlap)
   â†“
Gemini Embedding (3072-dim vector)
   â†“
Pinecone (Dense Vector Storage)
```

Indexing is done only once unless:

* Document changes
* Chunk size changes
* Embedding model changes
* Index is reset

---

### ğŸ”¹ Stage 2: Query (RAG Pipeline)

```
User Question
   â†“
Generate Embedding (Gemini)
   â†“
Pinecone Similarity Search (topK = 3)
   â†“
Retrieve Relevant Chunks
   â†“
Send Context + Question to Groq LLM
   â†“
Final Grounded Answer
```

---

## ğŸ“‚ Project Structure

```
app/
 â”œâ”€â”€ page.tsx              â†’ Chat UI
 â””â”€â”€ api/
      â””â”€â”€ llmchat/route.ts â†’ RAG API

scripts/
 â”œâ”€â”€ prepare.ts            â†’ Indexing script
 â”œâ”€â”€ embedding.ts          â†’ Gemini embedding logic
 â””â”€â”€ vectorStore.ts        â†’ Pinecone operations

data/documents/
 â””â”€â”€ nexus.pdf             â†’ Company knowledge base
```

---

## âš™ï¸ API Details

### POST `/api/llmchat`

Request:

```json
{
  "question": "What is the leave policy?"
}
```

Flow:

1. Create embedding for question
2. Query Pinecone
3. Build context
4. Call Groq LLM
5. Return response

Response:

```json
{
  "reply": "The company provides 18 paid leaves per year..."
}
```

---

## ğŸ” Environment Variables

Create `.env.local`:

```
GEMINI_API_KEY=your_gemini_key
PINECONE_API_KEY=your_pinecone_key
GROQ_API_KEY=your_groq_key
```

---

## â–¶ï¸ Running the Project

### 1ï¸âƒ£ Install Dependencies

```bash
npm install
```

### 2ï¸âƒ£ Run Development Server

```bash
npm run dev
```

Open:

```
http://localhost:3000
```

---

## ğŸ§ª Example Queries

* What is the leave policy?
* Who is the backend team lead?
* What are the working hours?
* How to bake a cake? â†’ (Should refuse)

---

## ğŸ› ï¸ Key Challenges Faced

### 1ï¸âƒ£ Gemini SDK Version Issues

* Confusion between `v1` and `v1beta`
* Model naming differences
* Embedding vs generateContent format mismatch

**Resolution:**
Used Gemini only for embeddings and switched to Groq for generation.

---

### 2ï¸âƒ£ Pinecone Query Errors

* Incorrect vector shape passed
* Upsert format mismatch
* Namespace handling issues

**Resolution:**
Ensured embedding returns `number[]` and used proper `query()` structure.

---

### 3ï¸âƒ£ Dependency Conflicts

* `dotenv` version conflict with LangChain dependency
* Peer dependency issues while installing groq-sdk

**Resolution:**
Removed unnecessary dotenv package (Next.js handles env internally).

---

### 4ï¸âƒ£ RAG Debugging Complexity

* Incorrect JSON destructuring in API route
* Passing object instead of string to embedding function
* SDK response shape mismatch

**Resolution:**
Strict TypeScript typing + console-based debugging.

---

## ğŸ¯ Key Learnings

* Importance of correct API versioning
* Understanding vector dimensions (3072 for Gemini embeddings)
* Prompt engineering for hallucination control
* Clean separation of:

  * Indexing logic
  * Retrieval logic
  * Generation logic
* Handling production-grade dependency conflicts

---

## ğŸ† Why This Project Is Important

This is not just a chatbot.

It demonstrates:

* RAG system design
* Vector database integration
* LLM grounding strategy
* Real-world debugging
* Production-level architecture thinking

This mirrors how enterprise AI assistants are built internally.

---

## ğŸš€ Future Improvements

* Add streaming responses
* Add conversational memory
* Add document upload via UI
* Add multi-document indexing
* Add citation references in responses
* Add authentication layer

---

## ğŸ“Œ Deployment

Can be deployed on:

* Vercel (Frontend + API)
* Railway / Render (if external hosting needed)

---

## ğŸ‘¨â€ğŸ’» Author

Built as a practice project to simulate real enterprise internal knowledge assistants.

---

---

If you want, I can now:

* ğŸ”¥ Make it even more recruiter-optimized (impact-focused version)
* ğŸ“Š Add architecture diagram (ASCII visual)
* ğŸ’¼ Add â€œWhat problems this solves for enterprisesâ€ section
* âœ¨ Add a portfolio-ready project summary paragraph for LinkedIn

Tell me which level you want.
